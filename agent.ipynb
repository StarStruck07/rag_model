{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.chat_models import init_chat_model\n",
    "import getpass\n",
    "import os\n",
    "import torch\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "llm = init_chat_model(\"llama3.1\",model_provider = \"Ollama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "loader = CSVLoader(\n",
    "    file_path = 'nirf_clean.csv',\n",
    "    source_column=\"Name\",\n",
    "    metadata_columns=[\"Rank\",\"State\"],\n",
    ")\n",
    "docs = loader.load()\n",
    "embed = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={\"device\": device}\n",
    "    )\n",
    "db = Chroma.from_documents(docs,embed,persist_directory=\"./chroma_db\")\n",
    "retiever = db.as_retriever(search_kargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.environ.get(\"TAVILY_API_KEY\"):\n",
    "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Tavily API key: \")\n",
    "from langchain_tavily import TavilySearch\n",
    "search_tool = TavilySearch(\n",
    "    max_results=5,\n",
    "    topic=\"general\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"history\",\n",
    "    input_key=\"input\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "template = \"\"\"\n",
    "You are an AI agent, specifically modeled to answer questions related to various engineering enterance exams in India.\n",
    "You should ask the user various questions related to his exam scores, branch preferences, interests etc to access his profile, and then reccomend him 3 colleges\n",
    "Refer to the list of top engineering colleges in India form the context from old database below. Also give the reccomendations based on the latests cutoffs from the web\n",
    "You must answer all the queires wihh the context provided below:\n",
    "Here is some relevant context form old database : {context}\n",
    "Here is some context taken from web: {web_context}\n",
    "Here is the chat history: {history}\n",
    "Here is the query : {input}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm,prompt)\n",
    "rag_chain = create_retrieval_chain(retiever,document_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from accelerate import init_empty_weights\n",
    "\n",
    "print(\"Hello! I am a llm agent to help with all your doubts regarding the various diffrent engineering entrance exams in India.\" \\\n",
    "\"Type 'exit' to quit\")\n",
    "while True:\n",
    "    user_input = input(\"\\nUser: \")\n",
    "    if user_input == \"exit\":\n",
    "        print(\"Exiting conversation.\")\n",
    "        break\n",
    "\n",
    "    db_results = retiever.invoke(user_input)\n",
    "    context = \"\\n\".join(res.page_content for res in db_results)\n",
    "\n",
    "    web_results = search_tool.invoke({\"query\":user_input})\n",
    "    raw = web_results['results']\n",
    "    web_context = [\n",
    "        Document(page_content=res['content'], metadata= {\"url\": res['url']}) for res in raw\n",
    "    ]\n",
    "    history_dict = memory.load_memory_variables({})\n",
    "    history = history_dict.get(\"history\", \"\")\n",
    "\n",
    "    response = rag_chain.invoke({\n",
    "        \"input\": user_input,\n",
    "        \"web_context\": web_context,\n",
    "        \"context\": context,\n",
    "        \"history\": history,\n",
    "    })\n",
    "    memory.save_context({\"input\": user_input},{\"output\":response['answer']})\n",
    "    print(f\"AI: {response['answer']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
